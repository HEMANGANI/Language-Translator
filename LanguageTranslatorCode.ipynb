{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageTranslatorCode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUF8gJAF1GCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8f1db3-bf14-4220-fb5f-47aea5edbf31"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "## From keras import \n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "#import tensorflow.python.keras.optimizers\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5iMwVtGr4kG"
      },
      "source": [
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcxXRhe04gQB"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMF-1lSr1GC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011fd2b6-5b6d-4a57-c007-af41a8c4a5bd"
      },
      "source": [
        "## Load the data\n",
        "#file1 = open(\"gdrive/My Drive/Colab Notebooks/train_en.txt\", encoding = \"utf8\")   # Load English Data\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "file1 = open(\"/content/train.en\", encoding=\"utf8\")\n",
        "english = file1.readlines()\n",
        "\n",
        "#file2 = open(\"gdrive/My Drive/Colab Notebooks/train_vi.txt\", encoding = \"utf8\")   # Load Vitnm Data\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "file2 = open(\"/content/train.vi\", encoding=\"utf8\")\n",
        "vitn = file2.readlines()\n",
        "\n",
        "### Now add a start and end marker for the destination language. \n",
        "for i in range(0,len(vitn)):\n",
        "    vitn[i] = \"starttt \" + vitn[i] + \" enddd\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-06 08:14:15--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13603614 (13M) [text/plain]\n",
            "Saving to: ‘train.en’\n",
            "\n",
            "train.en            100%[===================>]  12.97M  4.51MB/s    in 2.9s    \n",
            "\n",
            "2021-04-06 08:14:18 (4.51 MB/s) - ‘train.en’ saved [13603614/13603614]\n",
            "\n",
            "--2021-04-06 08:14:18--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18074646 (17M) [text/plain]\n",
            "Saving to: ‘train.vi’\n",
            "\n",
            "train.vi            100%[===================>]  17.24M  1.17MB/s    in 25s     \n",
            "\n",
            "2021-04-06 08:14:43 (714 KB/s) - ‘train.vi’ saved [18074646/18074646]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1phRPPl1GDF"
      },
      "source": [
        "num_words = 10000 ## Most frequent 10,000 words for tokenizing. Make it 30k if dataset is large\n",
        "\n",
        "class TokenizerWrap(Tokenizer):\n",
        "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "        \"\"\"\n",
        "        :param texts: List of strings. This is the data-set.\n",
        "        :param padding: Either 'post' or 'pre' padding.\n",
        "        :param reverse: Boolean whether to reverse token-lists.\n",
        "        :param num_words: Max number of words to use.\n",
        "        \"\"\"\n",
        "\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "    \n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "          \n",
        "            truncating = 'post'\n",
        "\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        " \n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
        "\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "\n",
        "            truncating = 'post'\n",
        "\n",
        "        if padding:\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding='pre',\n",
        "                                   truncating=truncating)\n",
        "        return tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBGsoA6u1GDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fede807b-0fa4-4a26-b3f3-cda24ba82ed5"
      },
      "source": [
        "### Now tokenize the datasets\n",
        "\n",
        "tokenizer_eng = TokenizerWrap(texts=english,\n",
        "                              padding='pre',\n",
        "                              reverse=True,\n",
        "                              num_words=num_words)     \n",
        " \n",
        "tokenizer_vitn = TokenizerWrap(texts=vitn,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)\n",
        "\n",
        "### This is to reduce the memory used for tokenizing the words.\n",
        "tokens_eng = tokenizer_eng.tokens_padded\n",
        "tokens_vitn = tokenizer_vitn.tokens_padded\n",
        "print(tokens_eng.shape)\n",
        "print(tokens_vitn.shape)\n",
        "## Tokenizing done here ####\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(133317, 44)\n",
            "(133317, 59)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S1MEQHD1GDQ"
      },
      "source": [
        "encoder_input_data = tokens_eng\n",
        "\n",
        "decoder_input_data = tokens_vitn[:, :-1]       # They are in reverse format, so reverse them\n",
        "decoder_output_data = tokens_vitn[:, 1:]       # First 'start' marker is time stepped in output\n",
        "\n",
        "##### Neural Network #####\n",
        "\n",
        "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
        "\n",
        "embedding_size = 128\n",
        "state_size = 512\n",
        "\n",
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')\n",
        "\n",
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvvWvt3q1GDU"
      },
      "source": [
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "\n",
        "    # Connect all the GRU-layers.\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output = net\n",
        "    \n",
        "    return encoder_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39H93rtE1GDY"
      },
      "source": [
        "## If we use LSTM's instead of GRU's at this place, We cannot take output the way we have taken now.     \n",
        "encoder_output = connect_encoder()\n",
        "\n",
        "\n",
        "######### Similarly build the decoder\n",
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')\n",
        "\n",
        "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')\n",
        "\n",
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)\n",
        "\n",
        "decoder_dense = Dense(num_words, activation='linear', name='decoder_output')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_toSul1GDc"
      },
      "source": [
        "def connect_decoder(initial_state):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "\n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU-layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc_3Abp51GDg"
      },
      "source": [
        "\n",
        "######## Now connect all layers and create the model.\n",
        "    \n",
        "decoder_output = connect_decoder(initial_state=encoder_output)\n",
        "\n",
        "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
        "\n",
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output])\n",
        "\n",
        "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
        "                      outputs=[decoder_output])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAftvi-H1GDm"
      },
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss between y_true and y_pred.\n",
        "    \n",
        "    y_true is a 2-rank tensor with the desired output.\n",
        "    The shape is [batch_size, sequence_length] and it\n",
        "    contains sequences of integer-tokens.\n",
        "\n",
        "    y_pred is the decoder's output which is a 3-rank tensor\n",
        "    with shape [batch_size, sequence_length, num_words]\n",
        "    so that for each sequence in the batch there is a one-hot\n",
        "    encoded array of length num_words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the loss. This outputs a\n",
        "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "\n",
        "    # Keras may reduce this across the first axis (the batch)\n",
        "    # but the semantics are unclear, so to be sure we use\n",
        "    # the loss across the entire 2-rank tensor, we reduce it\n",
        "    # to a single scalar with the mean function.\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWnq_X902fb4",
        "outputId": "83c07de6-70be-40d6-c5d4-900b97e5f378"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICJ4M7OS1GDq"
      },
      "source": [
        "#### Compile the model\n",
        "\n",
        "optimizer = RMSprop(lr=1e-3)    ### Adam / Adagrad dosent work well with RNN's\n",
        "#decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "#model_train.compile(optimizer=optimizer, loss=sparse_cross_entropy, target_tensors=[decoder_target])\n",
        "model_train.compile(optimizer=optimizer, loss=sparse_cross_entropy,run_eagerly=None)\n",
        "\n",
        "### Callbacks\n",
        "path_checkpoint = '21_checkpoint.keras'\n",
        "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                      monitor='val_loss',\n",
        "                                      verbose=1,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_best_only=True)\n",
        "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=3, verbose=1)\n",
        "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
        "                                   histogram_freq=0,\n",
        "                                   write_graph=False)\n",
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint,\n",
        "             callback_tensorboard]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I64IwRX1GDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f86fc09-73d0-4479-d254-4447c2d3f226"
      },
      "source": [
        "####### Train the model\n",
        "\n",
        "x_data = {\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}\n",
        "\n",
        "y_data = {\n",
        "    'decoder_output': decoder_output_data\n",
        "}\n",
        "\n",
        "validation_split = 10000 / len(encoder_input_data)\n",
        "print (validation_split)\n",
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=512,\n",
        "                epochs=4,\n",
        "                validation_split=validation_split,\n",
        "                callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07500918862560664\n",
            "Epoch 1/4\n",
            "241/241 [==============================] - 342s 1s/step - loss: 3.2777 - val_loss: 2.4601\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.46013, saving model to 21_checkpoint.keras\n",
            "Epoch 2/4\n",
            "241/241 [==============================] - 322s 1s/step - loss: 2.3270 - val_loss: 2.2903\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.46013 to 2.29032, saving model to 21_checkpoint.keras\n",
            "Epoch 3/4\n",
            "241/241 [==============================] - 325s 1s/step - loss: 2.1401 - val_loss: 2.1593\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.29032 to 2.15932, saving model to 21_checkpoint.keras\n",
            "Epoch 4/4\n",
            "241/241 [==============================] - 328s 1s/step - loss: 1.9937 - val_loss: 2.0244\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.15932 to 2.02437, saving model to 21_checkpoint.keras\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd734120250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBDd9G0xWBB"
      },
      "source": [
        "def train():\n",
        "    validation_split = 10000 / len(encoder_input_data)\n",
        "    print (validation_split)\n",
        "\n",
        "    model_train.fit(x=x_data,\n",
        "                    y=y_data,\n",
        "                    batch_size=512,\n",
        "                    epochs=4,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks = callbacks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YoLzPN51GDz"
      },
      "source": [
        "mark_start = 'starttt'\n",
        "mark_end = 'enddd'\n",
        "token_start = tokenizer_vitn.word_index[mark_start.strip()]\n",
        "token_end = tokenizer_vitn.word_index[mark_end.strip()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNProe5JbeNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "41228bf0-4469-4cb3-d467-5f4cdd472f8c"
      },
      "source": [
        "##### Save the trained model in Colab\n",
        "model_train.save('training_model.h5')\n",
        "\"\"\"\n",
        "model_train.save('training_model.h5')\n",
        "from google.colab import files\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# create on Colab directory\n",
        "model_train.save('training_model.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'training_model.h5'})\n",
        "model_file.SetContentFile('training_model.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel_train.save('training_model.h5')\\nfrom google.colab import files\\n!pip install -U -q PyDrive\\nfrom pydrive.auth import GoogleAuth\\nfrom pydrive.drive import GoogleDrive\\nfrom google.colab import auth\\nfrom oauth2client.client import GoogleCredentials\\n\\nauth.authenticate_user()\\ngauth = GoogleAuth()\\ngauth.credentials = GoogleCredentials.get_application_default()\\ndrive = GoogleDrive(gauth)\\n\\n# create on Colab directory\\nmodel_train.save('training_model.h5')    \\nmodel_file = drive.CreateFile({'title' : 'training_model.h5'})\\nmodel_file.SetContentFile('training_model.h5')\\nmodel_file.Upload()\\n\\n# download to google drive\\ndrive.CreateFile({'id': model_file.get('id')})\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26jFJA6p1GD3"
      },
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "    \"\"\"Translate a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_eng.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_vitn.max_tokens\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "    \n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_vitn.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "     # Print the input-text.\n",
        "    print(\"Input text:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "    # Print the translated output-text.\n",
        "    print(\"Translated text:\")\n",
        "    print(output_text)\n",
        "    print()\n",
        "\n",
        "    # Optionally print the true translated text.\n",
        "    if true_output_text is not None:\n",
        "        print(\"True output text:\")\n",
        "        print(true_output_text)\n",
        "        print()\n",
        "    \n",
        "    return input_text, output_text, true_output_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4E4ORHI1GD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba92c39f-aa03-45c7-d113-ed30dceca7ea"
      },
      "source": [
        "idx = 7\n",
        "input_text, output_text, true_output_text = translate(input_text=english[idx],\n",
        "         true_output_text=vitn[idx])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "They wrote almost a thousand pages on the topic .\n",
            "\n",
            "\n",
            "Translated text:\n",
            " họ đã nói về một người khác nhau trong một người enddd\n",
            "\n",
            "True output text:\n",
            "starttt Họ viết gần 1000 trang về chủ đề này .\n",
            " enddd\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkmmRLw0MfnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7f02d7-a628-47de-fd7c-aec1d29abf86"
      },
      "source": [
        "translate(input_text = 'This was an amazing day')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "This was an amazing day\n",
            "\n",
            "Translated text:\n",
            " đây là một bức ảnh enddd\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This was an amazing day', ' đây là một bức ảnh enddd', None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBridAlXVFy9",
        "outputId": "e839cc88-8679-49f8-b61c-6532c795bf06"
      },
      "source": [
        "translate(input_text = 'This is a cat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "This is a cat\n",
            "\n",
            "Translated text:\n",
            " đây là một ví dụ enddd\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This is a cat', ' đây là một ví dụ enddd', None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvLYWGDlY9nV",
        "outputId": "13e1f5ba-c09d-4640-a7a7-b41d10710608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "translate(input_text = 'Our college is in trichy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "Our college is in trichy\n",
            "\n",
            "Translated text:\n",
            " chúng tôi đã đi vào những người enddd\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Our college is in trichy', ' chúng tôi đã đi vào những người enddd', None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5j5rndxU8Xv"
      },
      "source": [
        "def translate1(input_text, true_output_text=None):\n",
        "    \"\"\"Translate a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_eng.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_vitn.max_tokens\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "    \n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_vitn.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    # Print the input-text\n",
        "    \n",
        "    return input_text, output_text, true_output_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66U8Eql1GEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4114398a-f631-444e-87a8-1cde1076939d"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "sm = SmoothingFunction()\n",
        "score = sentence_bleu([output_text], true_output_text, smoothing_function=sm.method1)\n",
        "print(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.21648852948266123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPWiTPmNEtda"
      },
      "source": [
        "\"\"\"\n",
        "def test():\n",
        "    ### Load the data\n",
        "    file1 = open(\"gdrive/My Drive/Colab Notebooks/tst2012_en.txt\", encoding = \"utf8\")   # Load English Data\n",
        "    english_test = file1.readlines()\n",
        "\n",
        "    file2 = open(\"gdrive/My Drive/Colab Notebooks/tst2012_vi.txt\", encoding = \"utf8\")   # Load Vitnm Data\n",
        "    vitn_test = file2.readlines()\n",
        "\n",
        "    ### Now add a start and end marker for the destination language. \n",
        "    for i in range(0,len(vitn_test)):\n",
        "        vitn_test[i] = \"starttt \" + vitn_test[i] + \" enddd\"\n",
        "    \n",
        "    #count = 0\n",
        "    scores_list = []\n",
        "    for idx in range(0,20): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = translate1(input_text=english_test[idx],true_output_text=vitn_test[idx])\n",
        "        scor = sentence_bleu([output_text], true_output_text, smoothing_function=sm.method1)\n",
        "        scores_list.append(scor)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average = sum(scores_list)/ 20\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average\n",
        "   \"\"\"    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdtPLFozUbwX"
      },
      "source": [
        "#BLEU_average = test()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}